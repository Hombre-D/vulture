{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f73b9f4",
   "metadata": {},
   "source": [
    "LSTM based on: https://medium.com/mlearning-ai/the-classification-of-text-messages-using-lstm-bi-lstm-and-gru-f79b207f90ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb07e127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, explore and plot data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Text pre-processing\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Modeling\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Embedding, Dropout, GlobalAveragePooling1D, Flatten, SpatialDropout1D, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32a2a1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('preprocessed_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72069f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "functions = df['function'].values\n",
    "y = df['isVulnerable'].values\n",
    "\n",
    "functions_train, functions_test, y_train, y_test = train_test_split(functions, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f5298d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "tokenizer = Tokenizer(analyzer=wordpunct_tokenize)\n",
    "tokenizer.fit_on_texts(functions_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(functions_train)\n",
    "X_test = tokenizer.texts_to_sequences(functions_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da02ed64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   41 37081     1    10    91    12    91     4   122    12  1966     7\n",
      "     9    10  7691    12   538     6  1731    91     2  5138     5   382\n",
      "    14   538     2   101     5     8     1   538     2 17660     7     9\n",
      "   538     2 37082     6    15     3    86     1   538     2 17660     5\n",
      "   538     2 17660     6    35     3    11     8     1  2359     1   538\n",
      "     4 13530    28     9   555    14   538     2 23429     5  4799     1\n",
      "   538     4 13530     5    11 37083    14    91     2  8529     5     8\n",
      "     1   538     2 13882     7     9     8   108  2359     1   538     4\n",
      " 25100     7    38   115    28     9     8     1  2359     1   538     4\n",
      "  5431     7    38   118     7     9 25101     1   538     4  3686     5\n",
      "     8     1 13531     1   538     7    38  4094     7     9  6394    14\n",
      "   538     2  4800     4 13532     5    11    48     9   122    19     6\n",
      "  4094     3    10 15441    12 25102     6  1731    91     2 25103    13\n",
      " 25104     5   122    12 25105     6    91     2 25103    13 25104    13\n",
      "  3612     3  4799     1   538     4 13530     5   247    14 25102     2\n",
      "  2575     4    18   538     2 25106     4    50     1    10 37084    52\n",
      " 37085     1    91     5 37086     1 25105     5   538     2 21462     6\n",
      " 25107     3   538     2 13882     6   118     3    19     6 37087     1\n",
      "    91     5     8     1    19    62  4094     7    27   589     3    11\n",
      "    11    11    48     9    20 13883     3 25101     1   538     4  3686\n",
      "     5   538     2 13882     6   118     3 13883     6 13531     1   538\n",
      "     5     8     1  4094    38 13883     7     9  6394    14   538     2\n",
      "  4800     4 13532     5    11    48     8     1 13883    38    99     7\n",
      " 10509  8779   311   393  4176    75   541    64  8530    58  4799     1\n",
      "   538     4  3686     5 10199     1    91     5    11    48     9     8\n",
      "     1 21463     1    91     7    62    15     7     9     8     1 27587\n",
      "     1    91     7    38    15    65  4094    62 37088     1    91     4\n",
      "    18   538     2 25106     4    32     4    35     4    15     7     7\n",
      "     9 37089     1    91     4    15     5 25108     1    91     4    32\n",
      "     5 13884     1    91     5    11    48     9   538     2 13882     6\n",
      "   115     3    11    11    48 13884     1    91     5  4799     1   538\n",
      "     4  3686     5    11    11    11    48     9     8     1 37090     1\n",
      "    91     4 37091    28     9     8     1  2359     1   538     4  4971\n",
      "     7    51  2359     1   538     4  5431    28     9     8     1 37092\n",
      "     1   538     7    38  4094     7     9 37093     1    91     4   538\n",
      "     2  3527    13  1851    13  3612     4 37094     5    11    11    11\n",
      "    11   589    22   331    14   538     2   101     5 37095     1    91\n",
      "     5 37096     1    91     5 23430     1    91     4   118     5    11\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import pad_sequences\n",
    "\n",
    "maxlen = 1000\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b502318d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54796"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "total_words = len(word_index)\n",
    "\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eba0572e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training tensor:  (19595, 1000)\n",
      "Shape of testing tensor:  (4899, 1000)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of training tensor: ', X_train.shape)\n",
    "print('Shape of testing tensor: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9b11255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1000, 300)         16439100  \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 1000, 300)        0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               219648    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,658,877\n",
      "Trainable params: 16,658,877\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 12:47:02.048461: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-03 12:47:02.049613: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-03 12:47:02.050357: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "drop_value = 0.2\n",
    "n_dense = 24\n",
    "\n",
    "\n",
    "# Define parameter\n",
    "n_lstm = 128\n",
    "drop_lstm = 0.2# Define LSTM Model \n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "model.add(SpatialDropout1D(drop_lstm))\n",
    "model.add(LSTM(n_lstm, return_sequences=False))\n",
    "model.add(Dropout(drop_lstm))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b09d1b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam' , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb27c84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 12:47:02.329495: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-03 12:47:02.330751: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-03 12:47:02.331541: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-03 12:47:03.059637: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-03 12:47:03.061395: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-03 12:47:03.062456: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613/613 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.5001"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 12:55:41.927141: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-03 12:55:41.928258: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-03 12:55:41.929069: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613/613 [==============================] - 542s 882ms/step - loss: 0.6941 - accuracy: 0.5001 - val_loss: 0.6937 - val_accuracy: 0.4907\n",
      "Epoch 2/30\n",
      "613/613 [==============================] - 472s 770ms/step - loss: 0.6939 - accuracy: 0.5011 - val_loss: 0.6936 - val_accuracy: 0.4895\n",
      "Epoch 3/30\n",
      "613/613 [==============================] - 472s 771ms/step - loss: 0.6934 - accuracy: 0.4981 - val_loss: 0.6924 - val_accuracy: 0.5111\n",
      "Epoch 4/30\n",
      "613/613 [==============================] - 471s 769ms/step - loss: 0.6948 - accuracy: 0.4989 - val_loss: 0.6936 - val_accuracy: 0.4903\n",
      "Epoch 5/30\n",
      "613/613 [==============================] - 472s 770ms/step - loss: 0.6933 - accuracy: 0.5040 - val_loss: 0.6928 - val_accuracy: 0.4911\n",
      "Epoch 6/30\n",
      "613/613 [==============================] - 471s 768ms/step - loss: 0.6932 - accuracy: 0.5054 - val_loss: 0.6938 - val_accuracy: 0.4909\n",
      "154/154 [==============================] - 23s 150ms/step - loss: 0.6938 - accuracy: 0.4909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6937890648841858, 0.49091652035713196]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.backend import clear_session\n",
    "clear_session()\n",
    "\n",
    "\n",
    "num_epochs = 30\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=num_epochs,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks =[early_stop],\n",
    "                    verbose=True)\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5d875b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
